[PATHS]
# Paths to resources.
# Some of the paths' files contain a question mark (?) which replaces the crawler by the current date.

# Path where the article downloads are dumped.
# Always provide the path including the file name with the question mark.
downloads = ./dump/website_downloads_?.csv

# The crawler stores all the RSS feed items of all RSS feeds here.
# There is no check if a specific article is already present in the dump, so in general each article (RSS item) is stored many times.
# Always provide the path including the file name with the question mark.
rss_articles = ./dump/crawled_articles_?.csv

# Path where responses of all HTTP requests are dumped.
# If the RSS feed dump or the article download dump is erroneous this dump can be used to restore your data set.
# Always provide the path including the file name with the question mark.
requests = ./dump/requests_?.csv

# Path to feeds list
feeds_list = ./resources/example_feed_list.csv

# Path to log directory
log_dir = ./logs

# name of the logfile; will be appended by current date
log_file_name = log_default

[REQUESTS]
# Website request timeout in seconds
website_request_timeout = 5

# RSS feed request timeout in seconds
rss_feed_request_timeout = 3


[CRAWLING]
# Interval for crawling all rss feeds in seconds
rss_feed_crawl_period = 240

# Number of threads downloading the articles
number_download_worker = 3

# Number of iterations to collect the articles that are already present.
# If you set this to 0 all articles that are present in the RSS feeds will be downloaded
# which means really high trafic at the start of the crawler.
warmup_iterations = 2

# Minimum number of seconds between two requests to a domain
# We strongly discourage you from setting this to 0 as this would disable the throttle and you therefore might overload the server with requests.
throttle_velocity = 2


[ARTICLE_DOWNLOAD_PATTERN]
# number of downloads per article
number = 7

# delay between each download in seconds
delay = 600